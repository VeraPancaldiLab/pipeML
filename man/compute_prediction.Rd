% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/machine_learning.R
\name{compute_prediction}
\alias{compute_prediction}
\title{Compute Prediction Metrics for a Trained Machine Learning Model}
\usage{
compute_prediction(
  model,
  test_data,
  target,
  file.name = NULL,
  maximize = "Accuracy",
  return = F
)
}
\arguments{
\item{model}{A trained machine learning model (e.g., from \code{caret} or other model training functions).}

\item{test_data}{A matrix or data frame containing the testing dataset (features only).}

\item{target}{A character vector of true target values for the test data (the observed labels).}

\item{file.name}{A character string to specify the filename for saving the confusion matrix plot
(optional). If \code{NULL}, the plot is not saved.}

\item{maximize}{A character string indicating which metric to maximize when selecting the best
threshold for the confusion matrix. Options include "Accuracy", "Precision",
"Recall", "Specificity", "Sensitivity", "F1", or "MCC". Default is "Accuracy".}

\item{return}{Logical. Whether to return the results and generated plots.}
}
\value{
A list containing:
\item{Metrics}{A data frame with various performance metrics (Accuracy, Sensitivity, Specificity,
Precision, Recall, F1 score, MCC) for each threshold.}
\item{AUC}{A list containing the AUROC and AUPRC values.}
\item{Predictions}{A data frame with the predicted probabilities for each class (e.g., \code{yes} or \code{no}).}
}
\description{
This function computes prediction metrics for a given machine learning model, including
the confusion matrix, AUROC, AUPRC, and other performance metrics such as Accuracy, Sensitivity,
Specificity, Precision, Recall, F1 score, and MCC. The function also determines the optimal
classification threshold based on a chosen metric (e.g., Accuracy, F1, or AUROC) and generates
a confusion matrix plot.
}
\details{
This function first generates predictions for the test dataset using the trained machine learning model.
It then calculates performance metrics for a range of threshold values and selects the threshold that maximizes
the chosen metric (e.g., Accuracy, F1 score, etc.). The function returns the metrics for the best threshold,
including AUROC and AUPRC, and produces a confusion matrix plot that compares predicted versus actual labels.

The confusion matrix plot is saved as a PDF with the name \verb{Confusion_Matrix_<file.name>.pdf} if a valid
\code{file.name} is provided.
}
\seealso{
\code{\link[caret]{confusionMatrix}}, \code{\link[caret]{varImp}}, \code{\link[ggplot2]{ggplot}}
}
