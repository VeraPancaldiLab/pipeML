---
title: "pipeML"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{pipeML}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(pipeML)
library(caret)
```

This tutorial demonstrates how to use the `pipeML` package for training and testing machine learning models. It introduces the main functions of the pipeline and guides you through additional functions for visualization. 

Load data
```{r}
raw.counts = pipeML::raw.counts
traitData = pipeML::traitData
deconvolution = pipeML::deconvolution
```

Feature selection

Apply repeated feature selection using the Boruta algorithm:
```{r}
deconvolution$target = as.factor(traitData$Best.Confirmed.Overall.Response)
res_boruta <- feature.selection.boruta(
  data = deconvolution,
  iterations = 5,
  fix = FALSE,
  doParallel = FALSE,
  threshold = 0.8,
  file_name = "Test",
  return = FALSE
)
```

Inspect the results:
```{r}
head(res_boruta$Matrix_Importance)

cat("Confirmed features:\n", res_boruta$Confirmed)

cat("Tentative features:\n", res_boruta$Tentative)
```

To further assess tentative features, set `fix = TRUE` to rerun the selection and confirm or reject them:
```{r, eval = FALSE}
res_boruta <- feature.selection.boruta(
  data = deconvolution,
  iterations = 5,
  fix = TRUE,
  doParallel = FALSE,
  threshold = 0.8,
  file_name = "Test",
  return = FALSE
)
```

For faster execution, enable parallelization (ensure parameters `doParallel` and `workers` are set):
```{r, eval = FALSE}
res_boruta <- feature.selection.boruta(
  data = deconvolution,
  iterations = 10,
  fix = FALSE,
  doParallel = FALSE,
  workers = 2,
  threshold = 0.8,
  file_name = "Test",
  return = FALSE
)
```

Train Machine Learning models (with optional stacking and feature selection)

Train and tune models using repeated stratified k-fold cross-validation:
```{r}
deconvolution = pipeML::deconvolution
traitData = pipeML::traitData
res <- compute_features.training.ML(features_train = deconvolution, 
                                    target_var = traitData$Best.Confirmed.Overall.Response,
                                    trait.positive = "CR",
                                    metric = "AUROC",
                                    stack = FALSE,
                                    k_folds = 2,
                                    n_rep = 2,
                                    feature.selection = FALSE,
                                    seed = 123,
                                    LODO = FALSE,
                                    batch_id = NULL,
                                    file_name = "Test",
                                    ncores = 2,
                                    return = FALSE)
```

Access the best-trained model:
```{r, eval = FALSE}
res[[1]]$Model
```

View all trained and tuned machine learning models:
```{r, eval = FALSE}
res[[1]]$ML_Models
```

To apply model stacking, set `stack = TRUE`:
```{r}
res <- compute_features.training.ML(features_train = deconvolution, 
                                    target_var = traitData$Best.Confirmed.Overall.Response,
                                    trait.positive = "CR",
                                    metric = "AUROC",
                                    stack = TRUE,
                                    k_folds = 2,
                                    n_rep = 2,
                                    feature.selection = FALSE,
                                    seed = 123,
                                    LODO = FALSE,
                                    batch_id = NULL,
                                    file_name = "Test",
                                    ncores = 2,
                                    return = FALSE)
```

Inspect the base models used in stacking:
```{r}
res$Model$Base_models
```

Access the meta-learner:
```{r, eval = FALSE}
res$Model$Meta_learner
```

To enable feature selection during model training, set `feature.selection = TRUE`. You must also specify the number of Boruta iterations via the `n_boruta` parameter and whether to resolve tentative features using the `boruta_fix` parameter (refer to `feature.selection.boruta()` for more details).
```{r}
res <- compute_features.training.ML(features_train = deconvolution, 
                                    target_var = traitData$Best.Confirmed.Overall.Response,
                                    trait.positive = "CR",
                                    metric = "AUROC",
                                    stack = TRUE,
                                    k_folds = 2,
                                    n_rep = 2,
                                    feature.selection = TRUE,
                                    n_boruta = 2, 
                                    boruta_fix = TRUE, 
                                    seed = 123,
                                    LODO = FALSE,
                                    batch_id = NULL,
                                    file_name = "Test",
                                    ncores = 2,
                                    return = FALSE)
```

When `feature.selection = TRUE`, the function will also return the features selected by Boruta. If set to `FALSE`, it returns all features by default.
```{r}
res$Features
```

To enable parallelization, specify the number of cores via the `ncores` parameter (default is `detectCores() - 1`).
```{r, eval = FALSE}
res <- compute_features.training.ML(features_train = deconvolution, 
                                    target_var = traitData$Best.Confirmed.Overall.Response,
                                    trait.positive = "CR",
                                    metric = "AUROC",
                                    stack = TRUE,
                                    k_folds = 3,
                                    n_rep = 3,
                                    feature.selection = FALSE,
                                    seed = 123,
                                    LODO = FALSE,
                                    batch_id = NULL,
                                    file_name = "Test",
                                    ncores = 3,
                                    return = FALSE)
```

If the training features are derived from multiple cohorts, you can instruct pipeML to incorporate this batch information into the k-fold construction. Set `LODO = TRUE` and provide the column name identifying the cohorts using the `batch_id` parameter. The function will then stratify folds to preserve cohort distribution across partitions.
```{r, eval = FALSE}
res <- compute_features.training.ML(features_train = deconvolution, 
                                    target_var = traitData$Best.Confirmed.Overall.Response,
                                    trait.positive = "CR",
                                    metric = "AUROC",
                                    stack = TRUE,
                                    k_folds = 3,
                                    n_rep = 3,
                                    feature.selection = FALSE,
                                    seed = 123,
                                    LODO = TRUE,
                                    batch_id = "cohort",
                                    file_name = "Test",
                                    ncores = 3,
                                    return = FALSE)
```

If you already have a separate testing dataset, you can directly train the model and perform predictions using the following function. You can also choose to optimize a specific metric to select a decision threshold and return detailed prediction performance metrics such as Accuracy, Sensitivity, Specificity, F1 score, MCC, Recall, and Precision.
```{r, eval = FALSE}
deconvolution = pipeML::deconvolution
data_train = deconvolution[1:100,]
data_test = deconvolution[!rownames(deconvolution) %in% rownames(data_train),]
res <- compute_features.ML(features_train = data_train, 
                           features_test = data_test, 
                           clinical = traitData,
                           trait = "Best.Confirmed.Overall.Response",
                           trait.positive = "CR",
                           metric = "AUROC",
                           stack = FALSE,
                           k_folds = 3,
                           n_rep = 3,
                           feature.selection = FALSE,
                           seed = 123,
                           LODO = FALSE,
                           batch_id = NULL,
                           file_name = "Test",
                           maximize = "F1",
                           return = FALSE)
```

You can generate ROC curve boxplots across multiple models:
```{r, eval = FALSE}
get_pooled_roc_curves(file.name = "Combined_Model", folder_path = "Results/Models/")
```

To compare model performance across different cohorts:
```{r, eval = FALSE}
get_pooled_boxplots(folder_paths = c("Results/Cohort1", "Results/Cohort2"), file_name = "TME_Comparison")
```

Access the prediction metrics:
```{r, eval = FALSE}
head(res$Prediction_metrics)
```

To compute variable importance:
```{r, eval = FALSE}
importance = compute_variable.importance(res$Model, stacking = FALSE, n_cores = 2)
```

And to visualize SHAP values:
```{r, eval = FALSE}
plot_shap_values(importance, "glm", "shap_plot")
```

